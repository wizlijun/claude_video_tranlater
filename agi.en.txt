1
00:00:00,330 --> 00:00:07,809
[Music]

2
00:00:14,960 --> 00:00:19,439
Hi everyone.

3
00:00:16,880 --> 00:00:21,439
That's a little loud. I'm Danielle and

4
00:00:19,439 --> 00:00:25,279
I'm a cognitive scientist working at the

5
00:00:21,439 --> 00:00:26,400
experimental new Amazon AGI SF lab. And

6
00:00:25,279 --> 00:00:28,240
throughout this conference, you're going

7
00:00:26,400 --> 00:00:30,320
to hear a lot of talks about building

8
00:00:28,240 --> 00:00:32,880
and scaling agents, including some from

9
00:00:30,320 --> 00:00:34,399
my colleagues at AWS. But this talk is

10
00:00:32,880 --> 00:00:36,320
going to be a little different. I want

11
00:00:34,399 --> 00:00:38,719
to think about how we can co-evolve with

12
00:00:36,320 --> 00:00:41,040
general purpose agents and what it will

13
00:00:38,719 --> 00:00:43,440
take to make them reliable and aligned

14
00:00:41,040 --> 00:00:46,320
with our own intelligence. So, I'd like

15
00:00:43,440 --> 00:00:49,039
to set the set the stage by reminding us

16
00:00:46,320 --> 00:00:52,000
of a fact about the reliability of our

17
00:00:49,039 --> 00:00:55,440
own minds. We're all hallucinating right

18
00:00:52,000 --> 00:00:57,680
now. Our brains don't have direct access

19
00:00:55,440 --> 00:00:59,120
to reality. So they're they're stuck

20
00:00:57,680 --> 00:01:01,760
inside our heads. So they can only

21
00:00:59,120 --> 00:01:04,799
really do a few things. They can make

22
00:01:01,760 --> 00:01:07,360
predictions with their world models.

23
00:01:04,799 --> 00:01:09,119
They can take in sensory information.

24
00:01:07,360 --> 00:01:11,680
And they can reconcile errors between

25
00:01:09,119 --> 00:01:13,920
the two. That's about it. And that's why

26
00:01:11,680 --> 00:01:15,840
neuroscientists call our brains

27
00:01:13,920 --> 00:01:19,119
prediction machines and say that

28
00:01:15,840 --> 00:01:21,360
perception is controlled hallucination.

29
00:01:19,119 --> 00:01:22,880
Uh, but there's no way, of course, that

30
00:01:21,360 --> 00:01:24,320
I could be standing up here in front of

31
00:01:22,880 --> 00:01:25,680
you if I didn't have my hallucinations

32
00:01:24,320 --> 00:01:27,920
under control. The controlled part is

33
00:01:25,680 --> 00:01:29,840
the critical bit. But that's not all

34
00:01:27,920 --> 00:01:32,240
that's happening right now. If you're

35
00:01:29,840 --> 00:01:34,880
understanding my words, then I'm also

36
00:01:32,240 --> 00:01:37,119
influencing your hallucinations. And

37
00:01:34,880 --> 00:01:39,119
assuming you do understand my words,

38
00:01:37,119 --> 00:01:41,439
then your brain just did something else.

39
00:01:39,119 --> 00:01:44,400
It activated all meanings of the word

40
00:01:41,439 --> 00:01:47,439
hallucination, including this one. So

41
00:01:44,400 --> 00:01:50,159
today we rely upon hallucinating chat

42
00:01:47,439 --> 00:01:52,799
bots for brainstorming, generating

43
00:01:50,159 --> 00:01:54,640
content and code and images of

44
00:01:52,799 --> 00:01:57,200
themselves like this. But what they

45
00:01:54,640 --> 00:01:59,520
can't yet do is think, learn, or act in

46
00:01:57,200 --> 00:02:01,439
a reliable general purpose way. And

47
00:01:59,520 --> 00:02:04,560
we're not satisfied with that because

48
00:02:01,439 --> 00:02:06,719
we've set our sights on building AI that

49
00:02:04,560 --> 00:02:08,239
more closely resembles our own

50
00:02:06,719 --> 00:02:11,280
intelligence.

51
00:02:08,239 --> 00:02:12,959
But what makes our intelligence general?

52
00:02:11,280 --> 00:02:14,800
Well, one thing we know is that

53
00:02:12,959 --> 00:02:17,520
hallucinations are necessary because

54
00:02:14,800 --> 00:02:20,400
they allow us to go beyond the data.

55
00:02:17,520 --> 00:02:22,800
They're features rather than bugs of AI

56
00:02:20,400 --> 00:02:24,959
that's flexible like ours. So, we just

57
00:02:22,800 --> 00:02:26,319
need to figure out how to control them.

58
00:02:24,959 --> 00:02:29,200
I'm going to be drawing a lot of

59
00:02:26,319 --> 00:02:30,800
parallels to our intelligence, but I'm

60
00:02:29,200 --> 00:02:33,280
not saying that we are or should be

61
00:02:30,800 --> 00:02:35,360
building something like a human brain.

62
00:02:33,280 --> 00:02:38,400
We don't want AI to replace us or

63
00:02:35,360 --> 00:02:41,360
replicate us. We want it to complement

64
00:02:38,400 --> 00:02:44,160
us. We want AI plus humans to be greater

65
00:02:41,360 --> 00:02:45,920
than the sum of our parts. Now, this

66
00:02:44,160 --> 00:02:48,640
isn't typically what we think about when

67
00:02:45,920 --> 00:02:51,280
we hear AGI. We think about the AI

68
00:02:48,640 --> 00:02:54,000
becoming more advanced. But this

69
00:02:51,280 --> 00:02:56,480
reflects a category error about how our

70
00:02:54,000 --> 00:02:59,120
intelligence actually works. And that

71
00:02:56,480 --> 00:03:02,239
error is that general intelligence can

72
00:02:59,120 --> 00:03:05,440
exist within a thinking machine. So when

73
00:03:02,239 --> 00:03:07,440
you think about AGI, you probably think

74
00:03:05,440 --> 00:03:09,680
about something like this. And you might

75
00:03:07,440 --> 00:03:12,640
think that it's right around the corner,

76
00:03:09,680 --> 00:03:17,239
but why does it then feel like agents

77
00:03:12,640 --> 00:03:17,239
are closer to something like this?

78
00:03:17,440 --> 00:03:22,959
The reality is that models can't yet

79
00:03:19,519 --> 00:03:25,120
reliably click, type, or scroll. And so

80
00:03:22,959 --> 00:03:27,280
everyone wants to know how do we make

81
00:03:25,120 --> 00:03:29,760
agents reliable? That's the question I'm

82
00:03:27,280 --> 00:03:33,200
going to focus on today. So first I'll

83
00:03:29,760 --> 00:03:35,200
share our lab's vision for agents.

84
00:03:33,200 --> 00:03:37,760
Then I will show you how Novaact which

85
00:03:35,200 --> 00:03:39,840
is a research preview of our agent works

86
00:03:37,760 --> 00:03:42,239
today. And then finally I'll show you

87
00:03:39,840 --> 00:03:45,280
how Novaact will evolve and how you are

88
00:03:42,239 --> 00:03:47,280
all central to that evolution. So let's

89
00:03:45,280 --> 00:03:49,200
start with the big picture. Our vision

90
00:03:47,280 --> 00:03:51,760
for agents is different than the

91
00:03:49,200 --> 00:03:53,360
standard vision which reflects this long

92
00:03:51,760 --> 00:03:55,760
lineage of thought that has become

93
00:03:53,360 --> 00:03:58,159
folklore. So you all know the story by

94
00:03:55,760 --> 00:04:00,879
now which is why you probably spotted

95
00:03:58,159 --> 00:04:02,799
the hallucination here. The concept of

96
00:04:00,879 --> 00:04:05,519
machines that can think like humans

97
00:04:02,799 --> 00:04:08,000
didn't originate in the 2010s, but in

98
00:04:05,519 --> 00:04:10,400
1956 when a group of engineers and

99
00:04:08,000 --> 00:04:11,599
mathematicians set out to build thinking

100
00:04:10,400 --> 00:04:14,080
machines so they could solve

101
00:04:11,599 --> 00:04:16,239
intelligence. Of course, you also all

102
00:04:14,080 --> 00:04:18,639
know that these guys didn't solve

103
00:04:16,239 --> 00:04:20,959
intelligence, but they did succeed in

104
00:04:18,639 --> 00:04:23,040
founding the field of AI and sparking a

105
00:04:20,959 --> 00:04:25,280
feedback loop that changed how we live

106
00:04:23,040 --> 00:04:27,360
and work. So first we built more

107
00:04:25,280 --> 00:04:28,800
powerful computers. Then we connected

108
00:04:27,360 --> 00:04:30,720
them together to build the internet

109
00:04:28,800 --> 00:04:32,960
which enabled more sophisticated

110
00:04:30,720 --> 00:04:34,880
learning algorithms. And this made our

111
00:04:32,960 --> 00:04:36,240
computers even more powerful. And now

112
00:04:34,880 --> 00:04:38,160
we're back to aiming for thinking

113
00:04:36,240 --> 00:04:41,040
machines by another name, artificial

114
00:04:38,160 --> 00:04:43,840
general intelligence or AGI. So the

115
00:04:41,040 --> 00:04:46,400
standard vision is to make AI smarter

116
00:04:43,840 --> 00:04:49,199
and give it more agency. And notice that

117
00:04:46,400 --> 00:04:50,720
this is about the technology, not us.

118
00:04:49,199 --> 00:04:53,040
Well, luckily this wasn't the only

119
00:04:50,720 --> 00:04:56,479
historical perspective. Does anybody

120
00:04:53,040 --> 00:04:58,320
know who this is?

121
00:04:56,479 --> 00:04:59,840
This is Douglas Angelbart and he

122
00:04:58,320 --> 00:05:02,080
invented the computer mouse and the

123
00:04:59,840 --> 00:05:03,759
guey. He didn't care so much about

124
00:05:02,080 --> 00:05:06,479
thinking machines and solving

125
00:05:03,759 --> 00:05:08,720
intelligence. What he cared about was

126
00:05:06,479 --> 00:05:10,880
thinking humans and augmenting our

127
00:05:08,720 --> 00:05:13,039
intelligence. And he proposed that

128
00:05:10,880 --> 00:05:15,600
computers could make us smarter. Of

129
00:05:13,039 --> 00:05:17,840
course, he was absolutely right. So, as

130
00:05:15,600 --> 00:05:20,160
computers became pervasive, they also

131
00:05:17,840 --> 00:05:24,000
started changing our brains. We began

132
00:05:20,160 --> 00:05:26,080
offloading our computation to devices,

133
00:05:24,000 --> 00:05:28,080
distributing our cognition across the

134
00:05:26,080 --> 00:05:31,120
digital environment. And this had the

135
00:05:28,080 --> 00:05:33,440
effect of augmenting our intelligence.

136
00:05:31,120 --> 00:05:35,680
Scientists call this technosocial

137
00:05:33,440 --> 00:05:38,160
co-evolution. It just means that we

138
00:05:35,680 --> 00:05:41,120
invent new technologies that then shape

139
00:05:38,160 --> 00:05:43,759
us. So here we have two historical

140
00:05:41,120 --> 00:05:45,199
perspectives for the goal of building

141
00:05:43,759 --> 00:05:48,080
more advanced intelligence that

142
00:05:45,199 --> 00:05:50,880
resembles our own. We can build AI that

143
00:05:48,080 --> 00:05:53,280
is as smart as or even smarter than us.

144
00:05:50,880 --> 00:05:55,680
Or we can build AI that makes us

145
00:05:53,280 --> 00:05:57,440
smarter. We all believe that more

146
00:05:55,680 --> 00:06:00,320
general purpose agents are going to be

147
00:05:57,440 --> 00:06:01,520
more useful. But how? Well, things are

148
00:06:00,320 --> 00:06:04,400
useful when they have one of two

149
00:06:01,520 --> 00:06:06,319
effects. They can simplify our lives by

150
00:06:04,400 --> 00:06:10,000
allowing us to offload things or they

151
00:06:06,319 --> 00:06:11,520
can give us more leverage. And yes,

152
00:06:10,000 --> 00:06:13,199
automation is an engine for

153
00:06:11,520 --> 00:06:15,440
augmentation. This is how we become

154
00:06:13,199 --> 00:06:17,919
expert at things. We start by paying

155
00:06:15,440 --> 00:06:19,840
conscious attention to the details. We

156
00:06:17,919 --> 00:06:22,800
practice and then our brain moves things

157
00:06:19,840 --> 00:06:24,960
over to our subconscious. Automation

158
00:06:22,800 --> 00:06:26,800
frees up our attention to focus on other

159
00:06:24,960 --> 00:06:29,120
things.

160
00:06:26,800 --> 00:06:31,360
The problem is that automation doesn't

161
00:06:29,120 --> 00:06:33,600
always lead to augmentation. Sometimes

162
00:06:31,360 --> 00:06:35,759
it even comes at a cost. How many hours

163
00:06:33,600 --> 00:06:37,600
have we lost to scrolling? Or how many

164
00:06:35,759 --> 00:06:40,240
echo chambers have we been trapped

165
00:06:37,600 --> 00:06:42,639
within? How many times has autocomplete

166
00:06:40,240 --> 00:06:45,680
just shut down our thinking? So this is

167
00:06:42,639 --> 00:06:47,440
how algorithms can reduce our agency.

168
00:06:45,680 --> 00:06:50,000
And it's how increasingly intelligent

169
00:06:47,440 --> 00:06:52,479
agents might cause more problems than

170
00:06:50,000 --> 00:06:54,800
they solve. But if we have precise

171
00:06:52,479 --> 00:06:56,800
control and we actively tailor these

172
00:06:54,800 --> 00:06:59,360
systems the way that we want, then we

173
00:06:56,800 --> 00:07:01,280
can actually increase our agency. And

174
00:06:59,360 --> 00:07:04,240
this is the crossroads in front of us.

175
00:07:01,280 --> 00:07:07,120
We can continue to make AI smarter and

176
00:07:04,240 --> 00:07:09,599
give it more agency. uh we can focus on

177
00:07:07,120 --> 00:07:11,599
unhobling the AI as it's fashionable to

178
00:07:09,599 --> 00:07:13,680
say, but this doesn't guarantee that it

179
00:07:11,599 --> 00:07:15,520
will be useful to us. It just guarantees

180
00:07:13,680 --> 00:07:17,039
that we'll continue to see a lot of the

181
00:07:15,520 --> 00:07:19,520
same patterns that we've seen in tech

182
00:07:17,039 --> 00:07:22,160
recently. And that's why that our vision

183
00:07:19,520 --> 00:07:25,120
is to build AI that makes us smarter and

184
00:07:22,160 --> 00:07:28,880
gives us more agency to build AI that

185
00:07:25,120 --> 00:07:31,440
unhobles humans. So, how do we do that?

186
00:07:28,880 --> 00:07:33,520
Well, in these early stages, we need to

187
00:07:31,440 --> 00:07:36,479
do two things. We need to meet the

188
00:07:33,520 --> 00:07:38,720
models where they are and meet the

189
00:07:36,479 --> 00:07:40,479
builders where they are. So all of you

190
00:07:38,720 --> 00:07:41,840
have a million ideas about what you want

191
00:07:40,479 --> 00:07:45,120
to do with agents. We have to make it

192
00:07:41,840 --> 00:07:48,240
frictionless for you to get started.

193
00:07:45,120 --> 00:07:51,280
And Nova Act does these two things.

194
00:07:48,240 --> 00:07:53,759
We're building a future where the atomic

195
00:07:51,280 --> 00:07:56,240
unit of all digital interactions will be

196
00:07:53,759 --> 00:07:58,479
an agent call. The big obstacle is that

197
00:07:56,240 --> 00:08:00,400
we still only have some infrastructure

198
00:07:58,479 --> 00:08:03,440
for APIs.

199
00:08:00,400 --> 00:08:06,639
Most websites are built for visual UIs

200
00:08:03,440 --> 00:08:09,440
and so since most websites lack APIs, we

201
00:08:06,639 --> 00:08:12,240
need to use the browser itself as a tool

202
00:08:09,440 --> 00:08:14,960
and that's why we've trained a model of

203
00:08:12,240 --> 00:08:17,360
uh Amazon's foundation model Nova to be

204
00:08:14,960 --> 00:08:21,440
really good at UIs to interact with UIs

205
00:08:17,360 --> 00:08:24,240
like we do. Nova Act combines this model

206
00:08:21,440 --> 00:08:27,280
with an SDK to allow developers to build

207
00:08:24,240 --> 00:08:29,520
and deploy agents. All you have to do is

208
00:08:27,280 --> 00:08:31,919
make an act call, which translates

209
00:08:29,520 --> 00:08:34,320
action uh natural language into actions

210
00:08:31,919 --> 00:08:36,959
on the screen. And I'm going to show you

211
00:08:34,320 --> 00:08:41,399
a demo here where my teammate Carolyn uh

212
00:08:36,959 --> 00:08:41,399
will show you how you can use Nova Act.

213
00:08:41,440 --> 00:08:46,160
Nova Act to find our dream apartment.

214
00:08:44,320 --> 00:08:49,440
We're searching for a two-bedroom, one

215
00:08:46,160 --> 00:08:51,680
bath in Redwood City. Here we've given

216
00:08:49,440 --> 00:08:53,120
our first act call to the agent. It's

217
00:08:51,680 --> 00:08:54,880
going to break down how to complete this

218
00:08:53,120 --> 00:08:57,360
task, considering the outcome of each

219
00:08:54,880 --> 00:08:59,360
step as it plans the next one. Behind

220
00:08:57,360 --> 00:09:01,360
the scenes, this is all powered by a

221
00:08:59,360 --> 00:09:03,360
specialized version of Amazon Nova

222
00:09:01,360 --> 00:09:06,360
trained for high reliability on UI

223
00:09:03,360 --> 00:09:06,360
tasks.

224
00:09:07,440 --> 00:09:11,760
And next, I'm going to show you my

225
00:09:09,200 --> 00:09:13,839
teammate Fjord, who will describe how

226
00:09:11,760 --> 00:09:16,399
you can uh do even more things with

227
00:09:13,839 --> 00:09:17,920
Python integrations. All right, we see a

228
00:09:16,399 --> 00:09:19,360
bunch of rentals on the screen. So,

229
00:09:17,920 --> 00:09:21,680
let's grab them using a structured

230
00:09:19,360 --> 00:09:23,760
extract. We'll define a pyantic class

231
00:09:21,680 --> 00:09:27,279
and ask the agent to return JSON

232
00:09:23,760 --> 00:09:28,720
matching that schema.

233
00:09:27,279 --> 00:09:30,399
For my commute, I want to know the

234
00:09:28,720 --> 00:09:32,480
biking distance to the nearest Cal Train

235
00:09:30,399 --> 00:09:34,640
station for each of these results. Let's

236
00:09:32,480 --> 00:09:36,399
define a helper function. Add biking

237
00:09:34,640 --> 00:09:38,080
distance will take in an apartment and

238
00:09:36,399 --> 00:09:41,040
then use Google Maps to calculate the

239
00:09:38,080 --> 00:09:42,320
distance.

240
00:09:41,040 --> 00:09:44,080
Now, I don't want to wait for each of

241
00:09:42,320 --> 00:09:45,920
these searches to complete one by one.

242
00:09:44,080 --> 00:09:47,920
So, let's do this in parallel. Since

243
00:09:45,920 --> 00:09:49,600
this is Python, we can just use a thread

244
00:09:47,920 --> 00:09:52,240
pool to spin up multiple browsers, one

245
00:09:49,600 --> 00:09:54,160
for each address.

246
00:09:52,240 --> 00:09:56,160
Finally, I'll use pandas to turn all

247
00:09:54,160 --> 00:09:59,760
these results into a table and sort by

248
00:09:56,160 --> 00:10:01,040
biking time to the cow train station.

249
00:09:59,760 --> 00:10:03,519
We've checked this script into the

250
00:10:01,040 --> 00:10:06,320
samples folder of our GitHub repo. So,

251
00:10:03,519 --> 00:10:08,480
feel free to give it a try.

252
00:10:06,320 --> 00:10:11,200
So, we've made it really easy to get

253
00:10:08,480 --> 00:10:12,800
started. It's just three lines of code.

254
00:10:11,200 --> 00:10:15,200
And under the hood, we're constantly

255
00:10:12,800 --> 00:10:17,040
making improvements to our model and

256
00:10:15,200 --> 00:10:19,200
shipping those every few weeks. And this

257
00:10:17,040 --> 00:10:21,680
is important. Because even the building

258
00:10:19,200 --> 00:10:24,320
blocks of computer use are deceptively

259
00:10:21,680 --> 00:10:26,160
challenging. Here's why. This is the

260
00:10:24,320 --> 00:10:29,040
Amazon website. And let me ask you, what

261
00:10:26,160 --> 00:10:30,480
do these icons mean? We typically take

262
00:10:29,040 --> 00:10:32,000
for granted that even if we've never

263
00:10:30,480 --> 00:10:33,920
seen them before, we can easily

264
00:10:32,000 --> 00:10:35,519
interpret them. Uh and and when we

265
00:10:33,920 --> 00:10:37,760
can't, there are usually plenty of cues

266
00:10:35,519 --> 00:10:39,920
for us to know what they mean. Now,

267
00:10:37,760 --> 00:10:41,920
Amazon actually labels these, but in

268
00:10:39,920 --> 00:10:43,920
many contexts, the icons are not

269
00:10:41,920 --> 00:10:45,600
labeled, and we couldn't possibly teach

270
00:10:43,920 --> 00:10:47,440
our agent all of the different icons,

271
00:10:45,600 --> 00:10:49,440
let alone all of the different useful

272
00:10:47,440 --> 00:10:51,360
ways that it could use a computer. So,

273
00:10:49,440 --> 00:10:53,920
we have to let our agent explore and

274
00:10:51,360 --> 00:10:56,160
learn with RL. And it's really

275
00:10:53,920 --> 00:10:58,720
fascinating to think about how RL will

276
00:10:56,160 --> 00:11:01,040
enable these agents to discover how to

277
00:10:58,720 --> 00:11:02,399
use computers in entirely new ways. And

278
00:11:01,040 --> 00:11:04,720
that's okay because we want them to be

279
00:11:02,399 --> 00:11:06,959
complimentary to us. But if we're going

280
00:11:04,720 --> 00:11:08,480
to diverge in our computer use methods,

281
00:11:06,959 --> 00:11:10,640
then it's really critical that our

282
00:11:08,480 --> 00:11:14,320
agents perception of the digital world

283
00:11:10,640 --> 00:11:16,880
is aligned with our own. And that's not

284
00:11:14,320 --> 00:11:20,079
what most agents can can do right now.

285
00:11:16,880 --> 00:11:22,640
So current agents are LLM rappers that

286
00:11:20,079 --> 00:11:24,160
function as readonly assistants. They

287
00:11:22,640 --> 00:11:26,399
can use tools and some of them are

288
00:11:24,160 --> 00:11:28,240
getting really good at code, but they

289
00:11:26,399 --> 00:11:30,320
don't have an environment to ground

290
00:11:28,240 --> 00:11:33,120
their interactions. They lack a world

291
00:11:30,320 --> 00:11:36,240
model. Computer use agents are

292
00:11:33,120 --> 00:11:38,880
different. They can see pixels and

293
00:11:36,240 --> 00:11:40,560
interact with UIs just like us. So you

294
00:11:38,880 --> 00:11:43,600
can think of them as kind of having this

295
00:11:40,560 --> 00:11:45,519
early form of embodiment. Now we're not

296
00:11:43,600 --> 00:11:48,079
the only ones working on computer use

297
00:11:45,519 --> 00:11:51,040
agents, but our approach is different.

298
00:11:48,079 --> 00:11:52,959
We are focusing on making the smallest

299
00:11:51,040 --> 00:11:55,839
units of interaction reliable and giving

300
00:11:52,959 --> 00:11:57,680
you granular control over them. Just

301
00:11:55,839 --> 00:12:00,000
like you can string together words to

302
00:11:57,680 --> 00:12:02,079
generate infinite combinations of

303
00:12:00,000 --> 00:12:04,560
meaning, you can string together atomic

304
00:12:02,079 --> 00:12:06,079
actions to generate increasingly complex

305
00:12:04,560 --> 00:12:08,480
workflows.

306
00:12:06,079 --> 00:12:11,440
Now, grounding our interactions in a

307
00:12:08,480 --> 00:12:13,920
shared environment uh is necessary for

308
00:12:11,440 --> 00:12:16,399
building aligned generalpurpose agents,

309
00:12:13,920 --> 00:12:18,480
but it's not sufficient. Computer use

310
00:12:16,399 --> 00:12:20,480
agents will need something else to be

311
00:12:18,480 --> 00:12:24,079
able to really reliably understand our

312
00:12:20,480 --> 00:12:26,880
higher level goals. So how will NOVA act

313
00:12:24,079 --> 00:12:28,399
need to evolve to make us smarter and

314
00:12:26,880 --> 00:12:30,480
give us more agency? In other words,

315
00:12:28,399 --> 00:12:33,440
what is it that makes our intelligence

316
00:12:30,480 --> 00:12:36,480
reliable and uh flexible and general

317
00:12:33,440 --> 00:12:39,040
purpose? Well, it turns out that over

318
00:12:36,480 --> 00:12:41,279
the past decades, as engineers were

319
00:12:39,040 --> 00:12:43,279
building more advanced intelligence,

320
00:12:41,279 --> 00:12:45,519
scientists were learning about how it

321
00:12:43,279 --> 00:12:48,240
works. And what they learned was that

322
00:12:45,519 --> 00:12:51,040
this isn't the whole story. It's just

323
00:12:48,240 --> 00:12:53,440
the most recent uh story of our

324
00:12:51,040 --> 00:12:55,920
co-evolution with technology. So

325
00:12:53,440 --> 00:12:58,639
computers co-evolving with computers is

326
00:12:55,920 --> 00:13:01,040
is this thing that we're fixated on. But

327
00:12:58,639 --> 00:13:03,760
the story goes back a lot longer. And

328
00:13:01,040 --> 00:13:06,800
Engelbart actually hinted at this. He

329
00:13:03,760 --> 00:13:08,079
said in a very real sense as represented

330
00:13:06,800 --> 00:13:10,000
by the steady evolution of our

331
00:13:08,079 --> 00:13:11,600
augmentation means the development of

332
00:13:10,000 --> 00:13:14,560
artificial intelligence has been going

333
00:13:11,600 --> 00:13:15,680
on for centuries. Now he was correct but

334
00:13:14,560 --> 00:13:17,200
it was actually going on for a lot

335
00:13:15,680 --> 00:13:19,600
longer than that. So let me take you

336
00:13:17,200 --> 00:13:21,519
back to the beginning. Around six

337
00:13:19,600 --> 00:13:23,760
million years ago, the environment

338
00:13:21,519 --> 00:13:26,399
changed for our ancestors and they had

339
00:13:23,760 --> 00:13:29,600
exactly two options. They could solve

340
00:13:26,399 --> 00:13:32,160
intelligence or go extinct. And the ones

341
00:13:29,600 --> 00:13:34,880
that solved intelligence did so through

342
00:13:32,160 --> 00:13:37,920
a feedback loop that changed our social

343
00:13:34,880 --> 00:13:40,160
cognition. This should look familiar.

344
00:13:37,920 --> 00:13:42,079
First, our brains got bigger. Then we

345
00:13:40,160 --> 00:13:44,160
connected them together, which enabled

346
00:13:42,079 --> 00:13:46,160
us to further fine-tune into social

347
00:13:44,160 --> 00:13:49,040
information. And this made our brains

348
00:13:46,160 --> 00:13:52,000
even bigger. But now you know that this

349
00:13:49,040 --> 00:13:54,160
scaling part is only half of the story.

350
00:13:52,000 --> 00:13:56,720
The other half had to do with how we all

351
00:13:54,160 --> 00:13:59,519
got smarter. So we offloaded our

352
00:13:56,720 --> 00:14:01,839
computation to each other's minds and

353
00:13:59,519 --> 00:14:03,600
distributed our cognition across the

354
00:14:01,839 --> 00:14:06,720
social environment. And this had the

355
00:14:03,600 --> 00:14:09,040
effect of augmenting our intelligence.

356
00:14:06,720 --> 00:14:11,120
So scientists call the thing that we got

357
00:14:09,040 --> 00:14:13,680
better at through these flywheels

358
00:14:11,120 --> 00:14:16,320
representational alignment. We figured

359
00:14:13,680 --> 00:14:19,519
out how to reproduce the contents of our

360
00:14:16,320 --> 00:14:21,680
minds to better cooperate. The key

361
00:14:19,519 --> 00:14:23,360
insight here is that the history of

362
00:14:21,680 --> 00:14:25,519
upgrading our intelligence didn't start

363
00:14:23,360 --> 00:14:28,240
with computers. It started with an

364
00:14:25,519 --> 00:14:31,199
evolutionary adaptation that allowed us

365
00:14:28,240 --> 00:14:32,560
to use each other's minds as tools. Let

366
00:14:31,199 --> 00:14:34,639
me say that in another way. The thing

367
00:14:32,560 --> 00:14:37,199
that makes our intelligence general and

368
00:14:34,639 --> 00:14:40,399
flexible is inferring the existence of

369
00:14:37,199 --> 00:14:42,959
other minds. This means that this is

370
00:14:40,399 --> 00:14:46,000
general intelligence. This can be

371
00:14:42,959 --> 00:14:48,320
general intelligence. This could

372
00:14:46,000 --> 00:14:50,000
possibly be general intelligence, but

373
00:14:48,320 --> 00:14:52,880
it's not uh there's no reason to expect

374
00:14:50,000 --> 00:14:55,360
that it will be aligned. And this is not

375
00:14:52,880 --> 00:14:58,480
general intelligence. Intelligence of

376
00:14:55,360 --> 00:15:00,560
the variety that humans have can't exist

377
00:14:58,480 --> 00:15:02,079
in a vacuum. It doesn't exist in

378
00:15:00,560 --> 00:15:05,120
individual humans. It won't exist in

379
00:15:02,079 --> 00:15:06,959
individual models. Instead, general

380
00:15:05,120 --> 00:15:09,839
intelligence emerges through our

381
00:15:06,959 --> 00:15:11,920
interactions. It's social, distributed,

382
00:15:09,839 --> 00:15:13,440
ever evolving. And that means that we

383
00:15:11,920 --> 00:15:15,680
need to measure the interactions and

384
00:15:13,440 --> 00:15:18,000
optimize for the interactions that we

385
00:15:15,680 --> 00:15:20,240
have with agents. We can't just measure

386
00:15:18,000 --> 00:15:22,079
model capabilities or things like time

387
00:15:20,240 --> 00:15:24,720
spent on platform. We have to measure

388
00:15:22,079 --> 00:15:27,199
human things like creativity,

389
00:15:24,720 --> 00:15:29,839
productivity, strategic thinking, even

390
00:15:27,199 --> 00:15:31,920
things like states of flow.

391
00:15:29,839 --> 00:15:34,160
So let's take a closer look at this

392
00:15:31,920 --> 00:15:36,959
evolutionary adaptation. Any ideas as to

393
00:15:34,160 --> 00:15:39,600
what it was?

394
00:15:36,959 --> 00:15:42,560
It was language. So, language co-evolved

395
00:15:39,600 --> 00:15:45,040
with our models of minds in yet another

396
00:15:42,560 --> 00:15:47,279
flywheel that integrated our systems for

397
00:15:45,040 --> 00:15:49,759
communication and representation. And it

398
00:15:47,279 --> 00:15:52,240
did this by being both a cause and an

399
00:15:49,759 --> 00:15:54,720
effect of modeling our minds. Let's

400
00:15:52,240 --> 00:15:56,560
break that down. We've got our models

401
00:15:54,720 --> 00:15:58,320
and our communicative interfaces. And

402
00:15:56,560 --> 00:16:02,079
then here's how they became integrated.

403
00:15:58,320 --> 00:16:05,040
As we fine-tuned into social cues, our

404
00:16:02,079 --> 00:16:07,440
models of mind became more stable. This

405
00:16:05,040 --> 00:16:09,440
advanced our language and our language

406
00:16:07,440 --> 00:16:12,480
made our models of mind even more

407
00:16:09,440 --> 00:16:14,720
stable. And then here's the big bang

408
00:16:12,480 --> 00:16:16,880
moment for our intelligence.

409
00:16:14,720 --> 00:16:19,519
Our models of mind became the original

410
00:16:16,880 --> 00:16:22,240
placeholder concept, the first variable

411
00:16:19,519 --> 00:16:25,199
for being able to represent any concept.

412
00:16:22,240 --> 00:16:26,480
That right there is generalization. So

413
00:16:25,199 --> 00:16:28,639
you might be thinking, but is this

414
00:16:26,480 --> 00:16:30,800
different from other languages? And the

415
00:16:28,639 --> 00:16:34,240
answer is yes. Other communication

416
00:16:30,800 --> 00:16:36,160
systems don't have models of mind.

417
00:16:34,240 --> 00:16:37,759
Programming languages don't negotiate

418
00:16:36,160 --> 00:16:41,440
meaning in real time. This is why code

419
00:16:37,759 --> 00:16:43,519
is so easily verifiable. And LLMs don't

420
00:16:41,440 --> 00:16:45,040
understand language. What do we mean

421
00:16:43,519 --> 00:16:47,120
they don't understand language? They

422
00:16:45,040 --> 00:16:49,600
don't understand that words refer to

423
00:16:47,120 --> 00:16:52,079
things that minds make up. So when we

424
00:16:49,600 --> 00:16:54,959
ask what's in a word, the answer is

425
00:16:52,079 --> 00:16:57,920
quite literally a mind.

426
00:16:54,959 --> 00:16:59,600
So language was so immensely useful that

427
00:16:57,920 --> 00:17:02,320
it triggered a whole new series of

428
00:16:59,600 --> 00:17:04,959
flywheels that scientists call cognitive

429
00:17:02,320 --> 00:17:07,280
technologies. Each one is a foundation

430
00:17:04,959 --> 00:17:09,520
for the next and each one allows us to

431
00:17:07,280 --> 00:17:12,799
have increasingly abstract thoughts.

432
00:17:09,520 --> 00:17:15,839
They become useful by evolving within

433
00:17:12,799 --> 00:17:17,760
communities. So early commu computers

434
00:17:15,839 --> 00:17:19,600
were not very useful to many people.

435
00:17:17,760 --> 00:17:22,799
They didn't have great interfaces. But

436
00:17:19,600 --> 00:17:25,679
Engelbart changed this. Now computers

437
00:17:22,799 --> 00:17:27,439
are getting in our way. We've never had

438
00:17:25,679 --> 00:17:29,360
the world's information so easily

439
00:17:27,439 --> 00:17:32,480
accessible, but also we've never had

440
00:17:29,360 --> 00:17:35,280
more distractions. And agents can help

441
00:17:32,480 --> 00:17:37,520
fix this. They can do the repetitive

442
00:17:35,280 --> 00:17:39,200
stuff for us. They can learn from us and

443
00:17:37,520 --> 00:17:41,280
redistribute our skills across

444
00:17:39,200 --> 00:17:43,679
communities. And they can teach us new

445
00:17:41,280 --> 00:17:45,840
things when they discover new knowledge.

446
00:17:43,679 --> 00:17:48,480
In essence, agents can become our

447
00:17:45,840 --> 00:17:50,799
collective subconscious. But we need to

448
00:17:48,480 --> 00:17:54,240
build them in a way that reflects this

449
00:17:50,799 --> 00:17:56,320
larger pattern. So collectively these

450
00:17:54,240 --> 00:17:58,320
tools for thought stabilize our

451
00:17:56,320 --> 00:18:00,960
thinking,

452
00:17:58,320 --> 00:18:03,280
reorganize our brains and control our

453
00:18:00,960 --> 00:18:05,360
hallucinations. How do they control our

454
00:18:03,280 --> 00:18:07,360
hallucinations? Well, they direct our

455
00:18:05,360 --> 00:18:09,280
attention to the same things in the

456
00:18:07,360 --> 00:18:11,039
environment. They pick out the relevant

457
00:18:09,280 --> 00:18:13,679
signals and the noise and then we

458
00:18:11,039 --> 00:18:16,080
stabilize these signals to co-create

459
00:18:13,679 --> 00:18:18,960
these shared world models. And what does

460
00:18:16,080 --> 00:18:20,480
that sound like? It sounds like what

461
00:18:18,960 --> 00:18:22,559
we're building. So another way of

462
00:18:20,480 --> 00:18:24,640
thinking about Nova Act is as the

463
00:18:22,559 --> 00:18:26,880
primitives for a cognitive technology

464
00:18:24,640 --> 00:18:28,960
that aligns agents and humans

465
00:18:26,880 --> 00:18:31,679
representations. And just like with

466
00:18:28,960 --> 00:18:35,200
other cognitive technologies, early

467
00:18:31,679 --> 00:18:37,280
agents will need to uh evolve in diverse

468
00:18:35,200 --> 00:18:40,320
communities. So that's where all of you

469
00:18:37,280 --> 00:18:42,160
come in. But reliability isn't just

470
00:18:40,320 --> 00:18:43,760
about clicking in the same place every

471
00:18:42,160 --> 00:18:46,240
time. It's about understanding the

472
00:18:43,760 --> 00:18:48,400
larger goal. So to return to our big

473
00:18:46,240 --> 00:18:50,880
question, how do we make agents

474
00:18:48,400 --> 00:18:54,000
reliable? Eventually they're going to

475
00:18:50,880 --> 00:18:56,400
need models of our minds. So the next

476
00:18:54,000 --> 00:18:58,480
thing that we'll need to build is agents

477
00:18:56,400 --> 00:19:00,320
with models of our minds. But we don't

478
00:18:58,480 --> 00:19:02,559
actually build those directly. We need

479
00:19:00,320 --> 00:19:05,200
to set the preconditions for them to

480
00:19:02,559 --> 00:19:07,280
emerge. And this requires a common

481
00:19:05,200 --> 00:19:08,559
language for humans and computers. And

482
00:19:07,280 --> 00:19:12,080
at this point, you know what this

483
00:19:08,559 --> 00:19:14,640
entails? Agents will need a a model of

484
00:19:12,080 --> 00:19:17,120
our shared environment and interfaces

485
00:19:14,640 --> 00:19:20,480
that support intuitive interactions with

486
00:19:17,120 --> 00:19:22,880
us. These will enable humans and agents

487
00:19:20,480 --> 00:19:25,360
to reciprocally level up one another's

488
00:19:22,880 --> 00:19:28,080
intelligence. To advance the models, we

489
00:19:25,360 --> 00:19:30,000
will need human agent interaction data.

490
00:19:28,080 --> 00:19:31,600
And to motivate people to use the agents

491
00:19:30,000 --> 00:19:33,840
in the first place, we'll need useful

492
00:19:31,600 --> 00:19:36,559
products. The more useful the products

493
00:19:33,840 --> 00:19:39,039
become, the smarter we will all become.

494
00:19:36,559 --> 00:19:42,320
So this is how we can collectively build

495
00:19:39,039 --> 00:19:44,320
useful general intelligence. Um if you

496
00:19:42,320 --> 00:19:46,160
want to learn more about Nova Act then

497
00:19:44,320 --> 00:19:50,760
stick around right here for the upcoming

498
00:19:46,160 --> 00:19:50,760
workshop. And thank you for your time.

499
00:19:52,160 --> 00:19:55,359
[Music]

